{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPXkUBs6eMHH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns, os, tqdm, re, sys, cv2, skimage, time\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "HEMRaE6mCK7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'my path to json file from kaggle in Google Disk'\n",
        "df = pd.read_csv(file_path); df"
      ],
      "metadata": {
        "id": "PiBC3BWkQ3T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_num_cols = df.select_dtypes(include=['float64']).columns\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "for i, col in enumerate(df_num_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(df[col].dropna(), kde=True, bins=30)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show();\n",
        "\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "y1V1qU8aOEeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh1X0Ck_v-RM"
      },
      "outputs": [],
      "source": [
        "df.price.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering."
      ],
      "metadata": {
        "id": "8xO61TVfOgxY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ3PZ4HRSxvS"
      },
      "outputs": [],
      "source": [
        "df_onehot = df.copy()\n",
        "\n",
        "df_onehot = pd.get_dummies(df_onehot, columns=['color'], drop_first=True)*1\n",
        "df_onehot = pd.get_dummies(df_onehot, columns=['clarity'], drop_first=True)*1\n",
        "df_onehot = pd.get_dummies(df_onehot, columns=['cut'], drop_first=True)*1\n",
        "\n",
        "df_onehot['y_log'] = np.log1p(df_onehot['y'])\n",
        "df_onehot['x_log'] = np.log1p(df_onehot['x'])\n",
        "df_onehot['z_log'] = np.log1p(df_onehot['z'])\n",
        "\n",
        "df_onehot['carat_log'] = np.log1p(df_onehot['carat'])\n",
        "df_onehot['depth_log'] = np.log1p(df_onehot['depth'])\n",
        "df_onehot['table_log'] = np.log1p(df_onehot['table'])\n",
        "\n",
        "df_onehot['proportion'] = df_onehot['depth'] / df_onehot['table']\n",
        "df_onehot['carat * proportion'] = df_onehot['carat'] * df_onehot['proportion']\n",
        "\n",
        "df_onehot['carat_squared'] = df_onehot['carat'] ** 2\n",
        "df_onehot['carat_table'] = df_onehot['carat'] * df_onehot['table']\n",
        "\n",
        "for color in ['color_E', 'color_F', 'color_G', 'color_H', 'color_I', 'color_J']:\n",
        "    df_onehot[f'carat_{color}'] = df_onehot['carat'] * df_onehot[color]\n",
        "\n",
        "for clarity in ['clarity_IF', 'clarity_SI1', 'clarity_SI2', 'clarity_VS1',\n",
        "                'clarity_VS2', 'clarity_VVS1', 'clarity_VVS2']:\n",
        "    df_onehot[f'carat_{clarity}'] = df_onehot['carat'] * df_onehot[clarity]\n",
        "\n",
        "for cut in ['cut_G', 'cut_I', 'cut_P', 'cut_V']:\n",
        "    df_onehot[f'carat_{cut}'] = df_onehot['carat'] * df_onehot[cut]\n",
        "\n",
        "df2 = df_onehot.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "aL6t8Twp5ppx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7OuVizOFsFF"
      },
      "outputs": [],
      "source": [
        "vX = df2.query('price!=price').drop(['price'], axis=1)  # slice a test sample\n",
        "tXY = df2.query('price==price')                       # slice training sample\n",
        "tX, tY = tXY.drop(columns=['price'], axis=1), tXY.price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAs2ayn-SjwH"
      },
      "outputs": [],
      "source": [
        "print(f'обучаяющая - {len(tXY)}, тестовая - {len(vX)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала попробуем разделить на обучающую и валиадционную выборки и посмотрим результаты."
      ],
      "metadata": {
        "id": "MfWzoE-SO0nW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nviw5N7GLTbn"
      },
      "outputs": [],
      "source": [
        "train_X, val_X, train_Y, val_Y = train_test_split(\n",
        "    tX, tY, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR"
      ],
      "metadata": {
        "id": "rt8d3Sw3PEf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
        "train_X_poly = poly.fit_transform(train_X)\n",
        "val_X_poly = poly.transform(val_X)\n",
        "vX_poly = poly.transform(vX)\n",
        "\n",
        "# 8.3: Стандартизация признаков\n",
        "scaler = RobustScaler()\n",
        "train_X_scaled = scaler.fit_transform(train_X_poly)\n",
        "val_X_scaled = scaler.transform(val_X_poly)\n",
        "vX_scaled = scaler.transform(vX_poly)"
      ],
      "metadata": {
        "id": "1JbioCk3_GH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(train_X_scaled, train_Y)\n",
        "val_pred = model.predict(val_X_scaled)\n",
        "\n",
        "# 10.2: Вычисление MAE на валидационной выборке\n",
        "mae = mean_absolute_error(val_Y, val_pred)\n",
        "print(f\"Mean Absolute Error (MAE) на валидационной выборке: {mae:.2f}\")\n",
        "\n",
        "train_pred = model.predict(train_X_scaled)\n",
        "\n",
        "# 10.2: Вычисление MAE на обучающей выборке\n",
        "mae_train = mean_absolute_error(train_Y, train_pred)\n",
        "print(f\"Mean Absolute Error (MAE) на обучающей выборке: {mae_train:.2f}\")"
      ],
      "metadata": {
        "id": "7nJeNNja_Az4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge"
      ],
      "metadata": {
        "id": "x6_m-872C54G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfsx_tKKbWnY"
      },
      "outputs": [],
      "source": [
        "ridge_model = Ridge(alpha=0.4)\n",
        "ridge_model.fit(train_X_scaled, train_Y)\n",
        "\n",
        "# Предсказание и оценка\n",
        "train_pred_ridge = ridge_model.predict(train_X_scaled)\n",
        "val_pred_ridge = ridge_model.predict(val_X_scaled)\n",
        "\n",
        "mae_train_ridge = mean_absolute_error(train_Y, train_pred_ridge)\n",
        "mae_val_ridge = mean_absolute_error(val_Y, val_pred_ridge)\n",
        "\n",
        "print(f\"Ridge Regression MAE на валидационной выборке: {mae_val_ridge:.2f}\")\n",
        "print(f\"Ridge Regression MAE на обучающей выборке: {mae_train_ridge:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но лучше делать эти проверки с помощью кросс-валидации."
      ],
      "metadata": {
        "id": "Yc6_hEbAEU4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('ridge', Ridge(alpha=0.01))\n",
        "])\n",
        "\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "scores = cross_val_score(pipeline, tX, tY, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "mae_scores = -scores\n",
        "\n",
        "print(f\"MAE по фолдам: {mae_scores}\")\n",
        "print(f\"Среднее MAE: {mae_scores.mean():.2f}\")\n",
        "print(f\"Стандартное отклонение MAE: {mae_scores.std():.2f}\")"
      ],
      "metadata": {
        "id": "fVq3uCQa0HhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соответственно тут уже обучаем и делаем предикт на тесте.\n",
        "- 1) Используем полиномы со второй степенью (если больше то все рушится).\n",
        "- 2) Вместо StandardScaler использовал Robust так как он менее чувствителен к выбросам, результат выдал мне лучше."
      ],
      "metadata": {
        "id": "uhnhN_rDEe9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_full = PolynomialFeatures(degree=2, include_bias=False)\n",
        "tX_poly_full = poly_full.fit_transform(tX)\n",
        "vX_poly_full = poly_full.transform(vX)\n",
        "\n",
        "robust_scaler = RobustScaler()\n",
        "tX_robust = robust_scaler.fit_transform(tX_poly_full)\n",
        "vX_robust = robust_scaler.transform(vX_poly_full)"
      ],
      "metadata": {
        "id": "cW_h-V4vBF5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью гридсерча нахожу оптимальный параметр альфа для нашего трансформированной обучающей выборки"
      ],
      "metadata": {
        "id": "py2ImkPqE1vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'alpha': [0.01, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "ridge_model = Ridge()\n",
        "grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
        "grid_search.fit(tX_robust, tY)\n",
        "\n",
        "print(f\"Best alpha: {grid_search.best_params_['alpha']}\")"
      ],
      "metadata": {
        "id": "N3OlWxYhxm6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем предикт"
      ],
      "metadata": {
        "id": "H-4q1dcnE-x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = Ridge(alpha=0.01) # альфа, который выдал greedsearch\n",
        "final_model.fit(tX_robust, tY)\n",
        "\n",
        "v_pred = final_model.predict(vX_robust)"
      ],
      "metadata": {
        "id": "knOePg17xqLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_price_bounds(carat_value):\n",
        "    if carat_value < 0.3:\n",
        "        min_price = tY[tX['carat'] < 0.3].min()\n",
        "        max_price = tY[tX['carat'] < 0.3].max()\n",
        "    elif carat_value < 0.5:\n",
        "        min_price = tY[(tX['carat'] >= 0.3) & (tX['carat'] < 0.5)].min()\n",
        "        max_price = tY[(tX['carat'] >= 0.3) & (tX['carat'] < 0.5)].max()\n",
        "    elif carat_value < 0.7:\n",
        "        min_price = tY[(tX['carat'] >= 0.5) & (tX['carat'] < 0.7)].min()\n",
        "        max_price = tY[(tX['carat'] >= 0.5) & (tX['carat'] < 0.7)].max()\n",
        "    elif carat_value < 1.0:\n",
        "        min_price = tY[(tX['carat'] >= 0.7) & (tX['carat'] < 1.0)].min()\n",
        "        max_price = tY[(tX['carat'] >= 0.7) & (tX['carat'] < 1.0)].max()\n",
        "    elif carat_value < 2.0:\n",
        "        min_price = tY[(tX['carat'] >= 1.0) & (tX['carat'] < 2.0)].min()\n",
        "        max_price = tY[(tX['carat'] >= 1.0) & (tX['carat'] < 2.0)].max()\n",
        "    else:\n",
        "        min_price = tY[tX['carat'] >= 2.0].min()\n",
        "        max_price = tY[tX['carat'] >= 2.0].max()\n",
        "    return min_price, max_price\n",
        "\n",
        "v_pred_clipped = []\n",
        "for i, pred in enumerate(v_pred):\n",
        "    carat_value = vX['carat'].iloc[i]\n",
        "    min_bound, max_bound = get_price_bounds(carat_value)\n",
        "    clipped_pred = np.clip(pred, min_bound, max_bound)\n",
        "    v_pred_clipped.append(clipped_pred)\n",
        "\n",
        "v_pred_clipped = np.array(v_pred_clipped)"
      ],
      "metadata": {
        "id": "PKVWuVVYGBkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame({\n",
        "    'id': range(1, len(v_pred_clipped) + 1),\n",
        "    'price': v_pred_clipped\n",
        "})\n",
        "output.head()"
      ],
      "metadata": {
        "id": "DRHZZN3dBL2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Готовый csv для выгрузки на kaggle."
      ],
      "metadata": {
        "id": "ANzZFfH1Rjn_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL-YpakRYutF"
      },
      "outputs": [],
      "source": [
        "output.to_csv('predict.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}